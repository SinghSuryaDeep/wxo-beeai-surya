# ğŸš— Predictive Maintenance Automation

**Enterprise AI Integration Demo: BeeAI + IBM watsonx.ai (Granite) + Watsonx Orchestrate + Langfuse**

[![IBM watsonx.ai](https://img.shields.io/badge/IBM-watsonx.ai-blue)](https://www.ibm.com/watsonx)
[![BeeAI Framework](https://img.shields.io/badge/BeeAI-Framework-green)](https://github.com/i-am-bee/bee-agent-framework)
[![Granite Models](https://img.shields.io/badge/Granite-3.8B-orange)](https://www.ibm.com/granite)
[![Watsonx Orchestrate](https://img.shields.io/badge/Watsonx-Orchestrate-purple)](https://www.ibm.com/watsonx/orchestrate)

---

## ğŸ“‹ Table of Contents

- [Overview](#-overview)
- [System Architecture](#-system-architecture)
- [Key Features](#-key-features)
- [Prerequisites](#-prerequisites)
- [Project Structure](#-project-structure)
- [Local Setup](#-local-setup)
- [IBM Code Engine Deployment](#-ibm-code-engine-deployment)
- [Watsonx Orchestrate Integration](#-watsonx-orchestrate-integration)
- [Observability with Langfuse](#-observability-with-langfuse)
- [Testing](#-testing)
- [API Reference](#-api-reference)

---

## ğŸ¯ Overview

This project demonstrates a **production-ready AI agent integration** for predictive vehicle maintenance, combining five enterprise technologies:

| Technology | Role |
|------------|------|
| **ğŸ¤– BeeAI Framework** | Agentic AI with tool orchestration |
| **ğŸ§  IBM watsonx.ai** | Enterprise LLM platform and infrastructure |
| **ğŸ’ Granite 3.8B** | High-performance instruction-following model |
| **ğŸ”— Watsonx Orchestrate** | Workflow automation and agent management |
| **ğŸ“Š Langfuse** | End-to-end AI observability and tracing |

### What This System Does

- âœ… **Predicts** vehicle component failures before they happen
- âœ… **Analyzes** real-time data: vehicle location, driver availability, service slots, parts inventory
- âœ… **Orchestrates** end-to-end workflows: predict â†’ estimate cost â†’ order parts â†’ book service â†’ notify driver
- âœ… **Integrates** WXO native agents with external BeeAI agent over HTTP
- âœ… **Traces** every agent decision, tool call, and LLM interaction

---

## ğŸ—ï¸ System Architecture

![Architecture Diagram](docs/architecture-diagram.gif)

### Data Flow

```
User Query: "Check maintenance for TRUCK-22"
    â†“
Watsonx Orchestrate (maintenance_agent)
    â”œâ”€â†’ Calls: BeeAI external agent (HTTP POST /chat/completions)
    â”‚     â†“
    â”‚   BeeAI Service (FastAPI)
    â”‚     â”œâ”€â†’ Tool 1: get_vehicle_location("TRUCK-22") â†’ San Francisco
    â”‚     â”œâ”€â†’ Tool 2: get_driver_schedule("driver-1") â†’ Available 2025-11-22 14:00
    â”‚     â”œâ”€â†’ Tool 3: get_dealership_slots("San Francisco") â†’ Slot 2025-11-22 15:00
    â”‚     â”œâ”€â†’ Tool 4: get_parts_inventory("Brake Pads") â†’ Stock: 5 units
    â”‚     â””â”€â†’ LLM Request â†’ IBM watsonx.ai (Granite 3.8B)
    â”‚           â””â”€â†’ Synthesizes comprehensive response
    â”‚
    â””â”€â†’ May trigger: predictive_maintenance_flow
          â”œâ”€â†’ predict_failure
          â”œâ”€â†’ check_maintenance_cost
          â”œâ”€â†’ order_parts
          â”œâ”€â†’ book_service_slot
          â””â”€â†’ notify_driver
    â†“
Langfuse captures full trace (all steps, timings, tokens, costs)
    â†“
User receives: Complete maintenance plan with booking details
```

### Integration Points

| From | To | Protocol | Description |
|------|-----|----------|-------------|
| WXO Agent | BeeAI Service | HTTP POST | External agent integration |
| BeeAI | watsonx.ai | REST API | LLM inference requests |
| All Services | Langfuse | OpenTelemetry | Trace collection |
| WXO Scheduler | WXO Flow | Internal | Recurring automation |

---

## âœ¨ Key Features

### BeeAI Service

- ğŸ¯ **Requirement Agent**: Enforces tool execution order
- ğŸ”§ **Tool Collection**: 4 predictive maintenance tools (location, schedule, slots, inventory)
- ğŸ’¾ **Memory Management**: Full conversation context retention
- ğŸ“ **Trajectory Logging**: Complete execution tracking
- ğŸŒ **WXO-Compatible API**: OpenAI-style `/chat/completions` endpoint

### IBM watsonx.ai & Granite Models

- ğŸ’ **Granite 3.8B Instruct**: High-performance instruction-following model optimized for enterprise workloads
- âš¡ **Low Latency**: ~200ms inference time for typical requests
- ğŸ”§ **Tool Calling**: Native support for function/tool invocation
- ğŸ“ **Context Window**: 8,192 tokens for comprehensive context understanding
- ğŸ’° **Cost-Effective**: Cheaper than GPT-3.5 and other LLMs with comparable performance
- ğŸŒ **Enterprise-Grade**: SLA guarantees, multi-region deployment, compliance-ready (GDPR, SOC2, HIPAA)

### Watsonx Orchestrate Integration

- ğŸ¤– **Native Agents**: maintenance_agent, scheduler_agent
- ğŸ”€ **Workflows**: 5-step predictive maintenance flow
- ğŸ”§ **Tools**: predict_failure, cost_estimation, parts_ordering, booking, notifications
- ğŸ”— **External Agent**: Calls BeeAI over HTTP with API key authentication
- â° **Scheduler**: Recurring maintenance checks (daily, weekly, cron-based)

### Observability

- ğŸ“Š **Full Tracing**: Every request, tool call, LLM interaction
- ğŸ’° **Cost Tracking**: Token usage and cost per request
- â±ï¸ **Performance**: Latency breakdown by component
- ğŸ” **Debug**: Step-by-step execution viewer
- ğŸ“ˆ **Analytics**: Usage trends, success rates, error patterns

---

## ğŸ“¦ Prerequisites

### Required Accounts

1. **IBM Cloud Account**
   - watsonx.ai service provisioned
   - Project created with Granite model access
   - API key generated

2. **Watsonx Orchestrate**
   - Subscription active
   - ADK installed: `pip install ibm-watsonx-orchestrate`
   - Authenticated: `orchestrate login`

3. **Langfuse (Optional)**
   - Free account at [cloud.langfuse.com](https://cloud.langfuse.com)
   - Project created
   - API keys obtained

### Development Tools

- Python 3.11+
- Docker or Podman
- Git
- curl (for testing)

---

## ğŸ“‚ Project Structure

```
.
â”œâ”€â”€ beeai_service/              # Main BeeAI service
â”‚   â”œâ”€â”€ config/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ settings.py         # Pydantic settings (env vars)
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ agent.py            # BeeAI RequirementAgent setup
â”‚   â”‚   â””â”€â”€ tools.py            # Tool definitions
â”‚   â”œâ”€â”€ servers/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ wxo_server.py       # FastAPI WXO-compatible server
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ __main__.py             # Entry point
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”œâ”€â”€ docker-compose.yml
â”‚   â”œâ”€â”€ pyproject.toml
â”‚   â”œâ”€â”€ setup_local.sh          # Local development setup
â”‚   â””â”€â”€ deploy_to_code_engine.sh # IBM Code Engine deployment
â”‚
â”œâ”€â”€ agents_observability/
â”‚   â””â”€â”€ langfuse_config.yml     # Langfuse configuration
â”‚
â”œâ”€â”€ wxo_tools/                  # Watsonx Orchestrate tools
â”‚   â”œâ”€â”€ predict_failure.py
â”‚   â”œâ”€â”€ maintenance_cost_tool.py
â”‚   â”œâ”€â”€ book_slot_tool.py
â”‚   â”œâ”€â”€ order_parts_tool.py
â”‚   â””â”€â”€ send_notification_tool.py
â”‚
â”œâ”€â”€ wxo_flows/                  # Watsonx Orchestrate flows
â”‚   â””â”€â”€ predictive_maintenance_flow.py
â”‚
â”œâ”€â”€ wxo_agents/                 # Watsonx Orchestrate agents
â”‚   â”œâ”€â”€ maintenance_agent.yaml
â”‚   â””â”€â”€ maintenance_scheduler_agent.yaml
â”‚
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ import_all.sh           # Import everything to WXO
â”‚
â”œâ”€â”€ docs/                       # Screenshots and diagrams
â”‚   â”œâ”€â”€ architecture-diagram
â”‚   â”œâ”€â”€ beeai-code-engine
â”‚   â”œâ”€â”€ beeai-local-startup
â”‚   â”œâ”€â”€ beeai-wxo-agent-details
â”‚   â”œâ”€â”€ langfuse-trace
â”‚   â”œâ”€â”€ wxo-agent-preview
â”‚   â””â”€â”€ wxo-import-agent-type
â”‚
â””â”€â”€ README.md                   # This file
```

---

## ğŸš€ Local Setup

### Step 1: Clone Repository

```bash
git clone <repository-url>
cd beeai_service
```

### Step 2: Configure Environment

Create `.env` file in `beeai_service/` directory:

```bash
# BeeAI Service Configuration
BEEAI_WXO_PORT=8080
BEEAI_WXO_HOST=0.0.0.0
BEEAI_API_KEY=beeai-maintenance-key-2024
BEEAI_LLM_MODEL=watsonx:ibm/granite-3-8b-instruct
BEEAI_LOG_LEVEL=INFO
BEEAI_LOG_INTERMEDIATE_STEPS=false

# IBM watsonx.ai (REQUIRED)
WATSONX_API_KEY=your_watsonx_api_key_here
WATSONX_URL=https://us-south.ml.cloud.ibm.com
WATSONX_PROJECT_ID=your_project_id_here
WATSONX_MODEL_ID=ibm/granite-3-8b-instruct
WATSONX_MAX_TOKENS=4096
WATSONX_TEMPERATURE=0.7
```

âš ï¸ **Important**: Replace `your_watsonx_api_key_here` and `your_project_id_here` with your actual credentials.

### Step 3: Start BeeAI Service

```bash
cd beeai_service
chmod +x setup_local.sh
./setup_local.sh
```

**Expected Output:**

![BeeAI Local Startup](docs/beeai-local-startup.png)

You should see:
- âœ… Container built and started
- âœ… Model loaded: `watsonx:ibm/granite-3-8b-instruct`
- âœ… Tools loaded: 4
- âœ… Agent initialized successfully
- âœ… Server running at `http://localhost:8080`

### Step 4: Test Local Deployment

#### Health Check

```bash
curl http://localhost:8080/health
```

**Expected Response:**
```json
{
  "status": "healthy",
  "service": "BeeAI Predictive Maintenance",
  "model": "watsonx:ibm/granite-3-8b-instruct",
  "timestamp": 1708312800
}
```

#### Test Agent (Streaming)

```bash
curl -X POST http://localhost:8080/chat/completions \
  -H "Content-Type: application/json" \
  -H "x-api-key: beeai-maintenance-key-2024" \
  -d '{
    "messages": [
      {
        "role": "user",
        "content": "Check maintenance status for vehicle TRUCK-22"
      }
    ],
    "stream": true
  }'
```

**Expected Response** (SSE stream):
```
data: {"id":"chatcmpl-beeai-xxx","choices":[{"delta":{"content":"Vehicle TRUCK-22 is currently located in San Francisco..."}}]}
...
data: [DONE]
```

### Step 5: View Logs

```bash
# Docker
docker compose logs -f

# Podman
podman compose logs -f
```

---

## â˜ï¸ IBM Code Engine Deployment

### Step 1: Update `.env` with IBM Cloud Credentials

Add these to your `.env` file:

```bash
# IBM Cloud Configuration
IBM_CLOUD_API_KEY=your_ibm_cloud_api_key
NAMESPACE=your_container_registry_namespace
IMAGE_NAME=beeai_maintenance_service
IMAGE_TAG=v1
APP_NAME=beeai-maintenance
PROJECT_ID=your_code_engine_project_id
RESOURCE_GROUP=your_resource_group
REGION=us-south
```

### Step 2: Deploy to Code Engine

```bash
cd beeai_service
chmod +x deploy_to_code_engine.sh
./deploy_to_code_engine.sh
```

**Deployment Steps Performed:**
1. âœ… Login to IBM Cloud
2. âœ… Build container image (linux/amd64)
3. âœ… Push to IBM Container Registry
4. âœ… Create/update Code Engine application
5. âœ… Configure environment variables
6. âœ… Set up health checks
7. âœ… Get public URL

#### View logs

```bash
ibmcloud ce app logs --name beeai-maintenance --follow
```

**Expected Output:**

![Code Engine Deployment](docs/beeai-code-engine.png)

### Step 3: Get Application URL

```bash
ibmcloud ce app get --name beeai-maintenance
```

**Example URL:**
```
https://beeai-maintenance.261z8nqmth9f.us-south.codeengine.appdomain.cloud
```

### Step 4: Test Deployed Service

```bash
curl -X POST https://beeai-maintenance.261z8nqmth9f.us-south.codeengine.appdomain.cloud/chat/completions \
  -H "Content-Type: application/json" \
  -H "x-api-key: beeai-maintenance-key-2024" \
  -d '{
    "messages": [
      {
        "role": "user",
        "content": "Check maintenance status for vehicle TRUCK-22"
      }
    ],
    "stream": true
  }'
```

### Useful Commands

```bash
# View logs
ibmcloud ce app logs --name beeai-maintenance --follow

# Get status
ibmcloud ce app get --name beeai-maintenance

# Scale
ibmcloud ce app update --name beeai-maintenance --min-scale 2 --max-scale 5

# Delete
ibmcloud ce app delete --name beeai-maintenance --force
```

---

## ğŸ”— Watsonx Orchestrate Integration

### Step 1: Authenticate with WXO Environment

First, activate your Watsonx Orchestrate environment:
```bash
orchestrate env activate <your_environment_name> --api-key <your_api_key>
```

**Example:**
```bash
orchestrate env activate EY_workflow_wxo --api-key azE6dXNyXzM0NWZjM2UyLTM5YmMtM2IzZC1iYjc5LTZjZWQ3ZGVhZWRlZjpwTkRoYlJxdHNpNTBRV2FveUdnUElCRFI4VlJ3SUxadEg3M0lxSUdzckpvPTpES0Ur
```

âš ï¸ **Note**: Replace `<your_environment_name>` and `<your_api_key>` with your actual WXO environment credentials.

### Step 2: Import All Components
```bash
cd scripts
chmod +x import_all.sh
./import_all.sh
```

This imports:
- âœ… 5 WXO Tools (predict, cost, order, book, notify)
- âœ… 1 Workflow (predictive_maintenance_flow)
- âœ… 2 Agents (maintenance_agent, scheduler_agent)
- âœ… Langfuse observability configuration

### Step 3: Register BeeAI as External Agent

#### WXO UI

1. **Navigate to**: Watsonx Orchestrate â†’ Agents â†’ Import agent

![Import Agent Type](docs/wxo-import-agent-type.png)

2. **Select**: External agent â†’ Next

3. **Configure Agent Details**:

![Agent Details](docs/beeai-wxo-agent-details.png)

| Field | Value |
|-------|-------|
| **External protocol** | External agent via chat completion |
| **Authentication type** | API key |
| **API key** | `beeai-maintenance-key-2024` |
| **External agent's URL** | `https://beeai-maintenance.xxx.codeengine.appdomain.cloud/chat/completions` |
| **Display name** | `beeai_predictive_maintenance_agent` |
| **Description** | An AI-powered predictive maintenance agent for fleet vehicles that analyzes maintenance requirements and provides real-time operational insights, including vehicle location tracking, driver availability and schedules, dealership service slot availability, and parts inventory verification. |

4. **Import agent** â†’ Done

### Step 4: Test Integration

**In WXO Chat:**
```
Can you help me check vehicle TRUCK-22
```

**Expected Flow:**

![WXO Agent Preview](docs/wxo-agent-preview.png)

The agent will:
1. âœ… Call BeeAI external agent
2. âœ… Receive real-time analysis (location, driver, slots, parts)
3. âœ… May trigger predictive_maintenance_flow
4. âœ… Return comprehensive maintenance plan with booking reference

**Example Response:**
```
Vehicle TRUCK-22 is currently located in San Francisco. The driver, driver-1, 
is available on 2025-11-22 from 14:00. The earliest available dealership slot 
for service is on 2025-11-22 at 15:00. The parts inventory for Brake Pads is 
available. 

Recommended action plan: Schedule the vehicle for service at the earliest 
available slot on 2025-11-22 at 15:00.

The service has been scheduled successfully.

Booking Reference: BOOK-TRUCK-22-2025-11-22T15-00-00
Vehicle: TRUCK-22
Service: Brake Pads maintenance (due in 7 days)
Scheduled Time: 2025-11-22 at 15:00 (local time)
Driver Notified: driver-1
```

---

## ğŸ“Š Observability with Langfuse

### Step 1: Configure Langfuse

1. **Create account**: [cloud.langfuse.com](https://cloud.langfuse.com)
2. **Create project**: "predictive-maintenance-fleet"
3. **Get credentials**: Settings â†’ API Keys

### Step 2: Update Configuration

Edit `agents_observability/langfuse_config.yml`:

```yaml
spec_version: v1
kind: langfuse
project_id: predictive-maintenance-fleet
api_key: "sk-lf-your-secret-key-here"
url: "https://cloud.langfuse.com/api/public/otel"
host_health_uri: "https://cloud.langfuse.com"
config_json:
  public_key: "pk-lf-your-public-key-here"
mask_pii: false
```

### Step 3: Import to WXO

```bash
orchestrate settings observability langfuse configure \
  --config-file=agents_observability/langfuse_config.yml
```

### Step 4: View Traces

**Langfuse Dashboard:**

![Langfuse Trace](docs/langfuse-trace.png)

**What Gets Captured:**
- âœ… User query
- âœ… Agent name and version
- âœ… All tool calls with parameters and responses
- âœ… LLM calls: input tokens, output tokens, cost
- âœ… Execution timing for each step
- âœ… Total latency
- âœ… Error traces (if any)

---

## ğŸ§ª Testing

### Test 1: Local Health Check

```bash
curl http://localhost:8080/health
```

**Expected**: HTTP 200 + JSON response with service status

### Test 2: Local Agent Test

```bash
curl -X POST http://localhost:8080/chat/completions \
  -H "Content-Type: application/json" \
  -H "x-api-key: beeai-maintenance-key-2024" \
  -d '{
    "messages": [{"role": "user", "content": "Check TRUCK-22"}],
    "stream": false
  }'
```

**Expected**: JSON response with vehicle analysis

### Test 3: Code Engine Test

```bash
curl -X POST https://your-app-url.codeengine.appdomain.cloud/chat/completions \
  -H "Content-Type: application/json" \
  -H "x-api-key: beeai-maintenance-key-2024" \
  -d '{
    "messages": [{"role": "user", "content": "Check TRUCK-22"}],
    "stream": false
  }'
```

**Expected**: JSON response from cloud deployment

### Test 4: WXO Integration

**In WXO Chat:**
```
Run a maintenance check for TRUCK-22
```

**Expected**:
- âœ… Agent calls BeeAI external agent
- âœ… Receives comprehensive analysis
- âœ… May trigger workflow
- âœ… Returns booking details

### Test 5: Scheduler

**In WXO Chat:**
```
Schedule daily maintenance checks for TRUCK-22 at 9am EST
```

**Verify:**
```bash
orchestrate schedules list
```

**Expected**:
- âœ… Schedule created
- âœ… Cron pattern: `0 9 * * *`
- âœ… Timezone: America/New_York

---

## ğŸ“š API Reference

### Endpoints

#### 1. Health Check

```
GET /health
```

**Response:**
```json
{
  "status": "healthy",
  "service": "BeeAI Predictive Maintenance",
  "model": "watsonx:ibm/granite-3-8b-instruct",
  "timestamp": 1708312800
}
```

#### 2. Agent Card (A2A Discovery)

```
GET /.well-known/agent-card.json
```

**Response:**
```json
{
  "name": "BeeAI Predictive Maintenance Agent",
  "description": "AI-powered vehicle maintenance analysis",
  "version": "1.0.0",
  "capabilities": {
    "streaming": true,
    "function_calling": false
  },
  "preferredTransport": "HTTP",
  "url": "http://localhost:8080"
}
```

#### 3. Chat Completions (Main Endpoint)

```
POST /chat/completions
```

**Headers:**
```
Content-Type: application/json
X-API-Key: beeai-maintenance-key-2024
```

**Request Body:**
```json
{
  "messages": [
    {
      "role": "user",
      "content": "Check maintenance for TRUCK-22"
    }
  ],
  "model": "watsonx:ibm/granite-3-8b-instruct",
  "stream": true
}
```

**Response (SSE):**
```
data: {"id":"chatcmpl-xxx","choices":[{"delta":{"content":"..."}}]}
...
data: [DONE]
```
---

## ğŸ“ Support & Resources

### Documentation

- **BeeAI Framework**: https://github.com/i-am-bee/bee-agent-framework
- **IBM watsonx.ai**: https://www.ibm.com/docs/en/watsonx
- **Watsonx Orchestrate**: https://www.ibm.com/docs/en/watsonx/watson-orchestrate
- **Langfuse**: https://langfuse.com/docs
- **Granite Models**: https://www.ibm.com/granite

### Getting Help

- **GitHub Issues**: Report bugs and request features
- **IBM Support**: For watsonx.ai/Orchestrate issues
- **Stack Overflow**: Tag `ibm-watsonx` or `beeai`

---

## ğŸ“„ License

Apache License 2.0

---

## ğŸ™ Acknowledgments

This project demonstrates enterprise AI integration patterns using:

- **IBM watsonx.ai** - Enterprise AI platform
- **Granite Models** - High-performance open LLMs
- **BeeAI Framework** - Agentic AI orchestration
- **Watsonx Orchestrate** - Workflow automation
- **Langfuse** - AI observability

---

**Built for enterprise AI integration**

*Showcasing: BeeAI + watsonx.ai + Granite + Watsonx Orchestrate + Langfuse*