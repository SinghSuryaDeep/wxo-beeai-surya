# ğŸš— Predictive Maintenance Automation

### Using BeeAI + Ollama + Watsonx Orchestrate + Scheduler + Agents Observability

This repository contains a complete **end-to-end predictive maintenance automation system** powered by:

* **BeeAI Framework (A2A agent with tool-calling)**
* **Ollama Granite 3.3 8B (local LLM runtime)**
* **Watsonx Orchestrate (flows, tools, agents)**
* **WXO Scheduler (recurring automation)**
* **Agents Observability (Langfuse Integration)**

The system can run:

### âœ” **Locally**

(For development, demos, offline usage, edge devices)

### âœ” **Inside Watsonx Orchestrate (SaaS)**

(For enterprise-grade scheduling, user interface, governance)

---

## Folder Structure

```
automotive_system/ 
â”œâ”€â”€ agents_observability/         # Agents observability configuration (Langfuse)
â”‚     â””â”€â”€ langfuse_config.yml
â”‚
â”œâ”€â”€ beeai_agent/                  # BeeAI Predictive Maintenance A2A server
â”‚     â”œâ”€â”€ __main__.py
â”‚     â”œâ”€â”€ tools_dummy.py
â”‚     â”œâ”€â”€ Dockerfile
â”‚     â””â”€â”€ pyproject.toml
â”‚
â”œâ”€â”€ beeai_host/                   # Simple BeeAI A2A client (local test tool)
â”‚     â”œâ”€â”€ __main__.py
â”‚     â”œâ”€â”€ Dockerfile
â”‚     â””â”€â”€ pyproject.toml
â”‚
â”œâ”€â”€ wxo_tools/                    # WXO Tools (Python)
â”‚     â”œâ”€â”€ predict_failure.py
â”‚     â”œâ”€â”€ order_parts_tool.py
â”‚     â”œâ”€â”€ book_slot_tool.py
â”‚     â”œâ”€â”€ maintenance_cost_tool.py
â”‚     â””â”€â”€ send_notification_tool.py
â”‚
â”œâ”€â”€ wxo_flows/
â”‚     â””â”€â”€ predictive_maintenance_flow.py
â”‚
â”œâ”€â”€ wxo_agents/
â”‚     â”œâ”€â”€ maintenance_agent.yaml
â”‚     â””â”€â”€ maintenance_scheduler_agent.yaml
â”‚
â”œâ”€â”€ scripts/
â”‚     â””â”€â”€ import_all.sh           # Import tools + Flows + Agents to WXO + Observability
â”‚
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ maintenance_flow.py
â”œâ”€â”€ maintenance_scheduler_agent.yaml
â””â”€â”€ Readme.md
```

---

## **Requirements**

### Local Requirements

* macOS / Linux / Windows WSL2
* Python 3.11+
* Ollama installed locally
* Granite model pulled:

```bash
ollama pull granite4:3b
```

Try out with other granite model â€” [https://ollama.com/library/granite4](https://ollama.com/library/granite4)

* BeeAI Framework:

```bash
pip install beeai-framework 'beeai-framework[a2a]'
```

### Watsonx Orchestrate Requirements

* Watsonx Orchestrate ADK installed:

```bash
pip install ibm-watsonx-orchestrate
```

* Access to Orchestrate workspace
* API key configured (`orchestrate login`)


### **Agents Observability (Langfuse Integration)**

This system includes **agent observability** using **Langfuse**, allowing you to track:

âœ” tool calls
âœ” model inputs/outputs
âœ” latency
âœ” errors
âœ” execution traces for AI Agents

#### **1. Configuration File**

The observability configuration is located here:

```
agents_observability/langfuse_config.yml
```

#### **2. Add Required Keys**

Update the file using your Langfuse project keys:

```yaml
api_key: "sk-lf-00000-00000-00000-00000-00000"
public_key: "pk-lf-00000-00000-00000-00000-00000"
```

#### **3. How It Works**

* BeeAI A2A server automatically loads the observability middleware.
* Every request/response, model call, and tool execution is reported.
* You can view insights in your **Langfuse dashboard**.

---

## **Part 1 â€” Run Locally (BeeAI + Ollama)**

### **1. Start Ollama**

```bash
ollama serve &
```

Test model:

```bash
ollama run granite3.3:8b "hello"
```

---

### **2. Start the BeeAI A2A Server**

```bash
cd automotive_system
python -m beeai_agent
```

Expected:

```
A2A server running on port 9999
Tools loaded: [...]
```

---

### **3. Test Using the BeeAI A2A Host**

```bash
cd automotive_system/beeai_host
python __main__.py TRUCK-22
```

You should see a complete maintenance summary.

---

## **Part 2 â€” Run in Watsonx Orchestrate (WXO)**

### **1. Import Everything**

```bash
cd automotive_system/scripts
./import_all.sh
```

Imports:

âœ” tools
âœ” flow
âœ” agents (on-demand + scheduled)

---

### **2. Interact With Agent in WXO UI**

```
Run a maintenance check for TRUCK-22
```

---

### **3. Schedule Maintenance**

```
Schedule a maintenance check for TRUCK-22 every day at 9am.
```

---

## **Troubleshooting**

### Agent says â€œvehicle not foundâ€

Ensure your tools have proper decorators:

```python
@tool(description="...")
```

### Server error: module not found

Run BeeAI from project root:

```
python -m beeai_agent
```

### Flow not visible in WXO

Check:

```
wxo_flows/predictive_maintenance_flow.py
```

### Scheduler not working

Ensure intrinsic tools are imported:

```
i__get_schedule_intrinsic_tool__
i__delete_schedule_intrinsic_tool__
i__get_flow_status_intrinsic_tool__
```

---

## **Conclusion**

You now have:

#### âœ” Predictive Maintenance LLM Agent (BeeAI + Granite)
#### âœ” End-to-End Workflow Automation (WXO Flow Builder)
#### âœ” Enterprise Scheduling (WXO Scheduler)
#### âœ” Local + Cloud Hybrid Setup
#### âœ” Full Observability with Langfuse

---
